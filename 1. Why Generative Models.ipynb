{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Generative Models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to provide motivation and intuition for why generative models exist, by showing how they relate to discriminative models (regressors, classifiers) that you may already be familiar with. We also explore why probabilistic models are a natural way to think about generation, we review standard methods to build generative models, and we end by talking about real-world applications of generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the way humans think, **discrimination** -- the ability to categorize and classify objects -- is usually very tightly linked to the ability to **generate** -- the ability to imagine or generate new kinds of objects. When you teach a child the digits 0 through 9, they are not only able to tell the different digits apart, but also conjure up images of the digits in their head when told to \"think of the digit 7.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, on a purely algorithmic level, a machine that is able to discriminate successfully does not necessarily mean that it can also generate well. Consider a simple example of a machine that is designed to discriminate between the handwritten images of the digits $0$ and $1$. An algorithm may do quite well on this task simply by looking at whether the center pixel of the image is included in the handwritten stroke -- if it is, the algorithm predicts $1$; otherwise, it predicts $0$. Such a simple algorithm, as shown below, is quite accurate at discrimination: it gets $99\\%$ accuracy discriminating between these two digits on the MNIST dataset. However, it hardly knows enough to generate new images of $0$ and $1$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeIAAABSCAYAAACIVNdiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAFIhJREFUeJzt3XlwVFXax/FvEglq3EC0wF0GcQmm0LhQqGSUsjQTcd9RGDcMoqIDlI4K4qjgFlxANOIuShRxiQIuo8O4Mo5REVzBJaOoqIzKjAGB5L5/9Puc2510J73dvg3z+1Sliup03+7Dvelzn3Oe85wCz/MQERGRcBSG/QFERET+l6kjFhERCZE6YhERkRCpIxYREQmROmIREZEQqSMWEREJkTpiERGREKkjFhERCZE6YhERkRBtkMs3KygoWKfLeHmeV9DRc9TG/Kc2RqiN+U9tjFjf26iIWEREJETqiEVEREKkjlhERCRE6ohFRERCpI5YREQkROqIRUREQpTT5UtBKCkp4cYbbwTg3HPPBaChoYETTjgBgMbGxtA+W6YOPPBAAN5880123XVXAI444ggAqqqqmD17dszz33jjDQBee+21HH5KSUVRUREAgwYNAmDUqFHcdtttAKxZs6bN819++WUAVqxYkaNPKLJ+O/HEEwF49NFH4/7++uuvB+DSSy/N2WdSRCwiIhKiAs/L3TrpIBZl9+rVi48++ijmscLCQi688EIAbr/99qy9V9ALzzfbbDMAHn74YQAOOeQQAFauXElxcTEAm2yyScLXr1y5EoCmpiaGDx8OwOOPP57SZwhrcX2XLl3o27cvAJWVlQCMGTOGlpYWwG+HjXDU1NSwbNmytN4rzAICG220EQD//e9/k3r+tGnTAKiurk7pfYJu40477QTAF198Ye9nxyTed8p9990HwNKlSwH48MMPgUhUku53UK7P48YbbwzAlltuybfffgvA2WefDcDYsWPp3r07ANdccw3gR1ZNTU1pv2dY1+pmm23mvnN+/PHHbB8+Rq7aeMoppwDw4IMPAv7oVJzPA8DatWsBqKurA+CDDz5gzpw5ACxatCil9+6ojetsR7zVVlsB8NRTT7HffvvF/G5d7YjvuOMOwB9ij2Y3Gz/88AMQO1RZUBD5WFVVVe6x//znPwAcdNBBALz//vtJfYZc/VF06tQJiAzNAowYMYIePXq0fp+EX9IPPvggZ555ZlrvHXQbS0pKACgvLwdg9erVAMyfPz/ljthuROx8V1VVMX/+/A5fl6uO+LPPPkv3EACMHDmSqVOnAn5bk5XrTsqmu2bMmMHcuXMB/6ax1XsCcMUVVwAwZcoU9/eYqly3ceTIkQCcd955bL755oB/Mzh79mx69eqV8LXvvvsuEOmwUpGrNn7yyScA7LLLLmkfw75/x40bB0BtbW1Sr1NlLRERkTy2zkXEFukeffTRgB/xRSssLHRDmZa4tGDBAgBeeeWVtN87yDu30tJS5s2bB0SGvgC+/vprAIYMGcKSJUsA+Pnnn4HYiKqwMHI/ZXdpV1xxhRt2eeKJJ4DIENpPP/3U4efI1d3p+eefD8Att9yS8DmvvPIKAwYMSPj7DTZIL9cw6DbW1NQAcNFFFwH+uTrzzDN5+umnAbjnnnsAOO2001I6dkVFRVLJeLmaRrERjQsuuADARVGp2HnnnQH417/+ldLrch0tXn755QBcddVVLuq178877rjDTSm9/vrrMb+bPHkyF198cVrvGXQbbfjZIrvTTz8d8L9TorW0tMR93DQ3NwMwadIkAG666SYXQbYn6DbeeuutQGTUDWLb9tVXXwFw1FFHucfsPB933HEJj7l8+XLAH5ntiCJiERGRfOZ5Xs5+AC/Tn+bmZq+5udlbs2ZNwp94v1+yZIm3ZMkSr7y8PO33DrKN/fr181paWryWlhbXxhEjRngjRoxI+VgTJkzwVq9e7a1evdods6qqKvQ2Al5paalXWlrqLVu2zFu2bJm3du3aNj+jR4/2Ro8e7XXq1MmbOHGiN3HixLjPy6fzWFJS4pWUlHg1NTXeqlWrvFWrVrnzaD+TJ092z6+srPQqKyu9BQsWuPa0fn68n4ULF+bFeYx3/fbr189rbm527Z8xY4Y3Y8YM74cffmi3TdXV1V51dXVenMd4P+Xl5V55ebnX1NTkNTU1xZyrYcOGecOGDfOKi4vd82tra73a2lp3Xuvq6vLqWrWfXr16eQ0NDV5DQ4P7nmjv5/vvv/fmz5/vzZ8/37v22mtjfl5//XVvxYoV3ooVK9zz6+vrvaKiIq+oqCi0Ng4dOtSdh9btmTt3rte1a1eva9euMa8pLi72iouLvb59+3p9+/b1Zs6c6c2cOTPmtXbMqVOnZuU8rjND05atZskR7SV2LF++3A0H7rjjjm1+nyhbriNBDqFUVFTwt7/9DYD7778fIO1kJPCTaGzY77777uOss87q8HVBD79PnDgR8BPL7PprbGzkyCOPBPzEtJaWFpfUtddeewFQX18PQLdu3VzmbVlZWUqfI4g2Tp48GYgkuSTy1ltvMXjwYAA+//xz97idbxsWtKG0ffbZhwMOOCDmGE1NTZx66qkAPPPMMwnfK1fDtnZ+Zs2aBUTO6/PPPw/AH/7wBwC23XZbNyxoQ7Q2JArw6aefAtCvXz8Afvnll6TeO1dtfOyxx4DYoUob3rQErujVCZtuuingT4OVlZUxZMgQwF8Rkawg2miJdvPmzWOHHXYA/GmB7777DoBnn33WTT/Y9NDJJ5/Ml19+mfC4vXv3BvypmaqqKq6++moArrzyyoSvC6KNJ598MgA33HAD2223XdznDBgwIKlpnm222QaITHPa/51ZtWoV++yzD+CvBIhHQ9MiIiJ5bJ2orFVRUeEqS1kkHC8ivvPOOwF44YUX3F21rcW1CXjArbG15UL5wO4cAf7xj39kfDyLSmz9qUUbYdp7771dJGwRhS3tmTp1atxlD1Zt6q233gL86HHUqFHsueeeANx1110ADBs2LLgPH8dGG23k7vTjLTlrbb/99uN3v/sdEBsR25KRKVOmAH5i4dZbb+2S7eyue+ONN3aJJe1FxEGzSHjs2LGAP8LR2NjIOeecE/PcpUuXctlllwF+OwYOHOh+b5GULe1KNiLOFRu1iR49XLVqFeAn7USzpUoW6e+5555uKVOqEXE2denSBcCNvO2www7u//r3v/894CeI2hpa8JMiox+Lx9prowRfffWVu7YtyfTmm2/OuB3t2W233QC49957Adhwww3bPOemm24C4J///GdSx/zmm2+ASFXDZ599FvBHFTbccEN37bcXEXdEEbGIiEiI8joitruOuro6unXrFvc5jY2Nbn7qqquuAmIr2VglJouWttpqK2644QbAv1uaMmVK3Dq/udCzZ08gMg9hd6cLFy7M+LhWozjVikxBqqysdFGFjWjYki2bV+qI1X+trKykT58+gB9l5dphhx3GmDFjkn7+okWL3HKJaHbe33777ZjHly5d6uaUrRgB4EaH7O+jvXm7oGy77bZA7EgTRIo/WPWseKzalNVR79y5s/vdMcccA+TXSFUitlzLost4rCLTcccdl1ERiWzo1KmT+w60vJk1a9Zw/PHHA+1fQx1Fwq3ZaMHw4cPd/Lr9fQcdEVueSbxI2KJ9y8H47bffUjr2hx9+6M73GWeckcnHbEMRsYiISIjyOiK2uYl40fDf//53IJId114tVIuILVt30qRJrmasRcb19fUZl+pLlxV06Nmzp4vsbRel9YUVKGldihTgoYceSuuYDz30kIuuwhKd+dsem0c86aST+PjjjzN+j/79+wN+tngYEbHNA5rWc/iJvPTSS4Bf9MJyOAD22GOPLH7CzFm2rGV/R7v77rs7fL3Vo84H/fv3d9+Bprq62p2PINi8cL6wTOr2RmzCktcdcTw2fGdLe5ItSG7LXgYPHsy+++4bzIdLg10cv/zyixsyWd9YzeXo1P9XX30VoM1WjumwJJQePXrk9Muvrq4uYS3saC+++CJAyp0wJN6qLUxlZWWuMpoNQ1oyUrL//1ZZLLojzje2fMdu3NNlVbjCFP3d8u9//xtI/yY4n9lUXzRLsHrnnXcCeU9LTrTteNOhoWkREZEQrRMRcXRt0P333z+tY9hdaWFhYZt6qePHj3fFFMLy8ccfJ7W4fF1kEXE0W/aTTP3rjmy//fYA9OnTJ6+GA5988knAT+xZX5x77rmuSMJf//pXgECHOMNikU6mRY9yWTQpkbKyMpcgaVM6qSZhpcqWuIG/TDFo8ZKorrvuOsAfvck2K4qSCUXEIiIiIcrriNiW3qS6T2k8gwYNAiKlElsXBRk/fnzGx0+V7Vkbfde4vrI5tui5Mku2y4SNbGTj+sgmK0xi8//JRh6WrGRzyl26dImbrGUbm2djfj0VtrOSLREBmD59ek4/Qy7Z+TO2/CWZ/aDz0cqVKwG/2E/QokcvLZkvaPa3EV3O10p0BpUEm43kr7zuiK3zTIdtT2VfblbZJ5pt0RXGGuITTzwRwFVaSjbpLFnRX5YQ/DBUeyw5LttDdNYB58PQXzT7PMn8n5911lnsvffegL+lZ/fu3RM+/9dff3VfNrbtXK7YKgbLJl7f2Xmw82kdcOsOel1QUFDgbv6T3bovXXb8Sy65xD2WbBWrTMVbmdGrV6+sHLu4uJiuXbu2edwqOmZCQ9MiIiIhyuuIOBNW8cd2fYlm6y6HDh0KpL4heb4rLy/niCOOiHks3ojA+sJ22opX9zdIBQUFcaNxq5kcr+KXJan17dsXgC222CKl5TGDBw9ut5rTusxqFeeLTJcdWf3mgoKCrEzFZItNdyS6ftNRVFTkdsa77bbbgEjVtO+//x7IXbW09957D8DVoc8Gq/42fvx4V+c92xQRi4iIhGi9jIjnzJnj6vHGY7tkrG/LhWyZ0J/+9Ce22GILwK9glKsEjVyx/V3BT7YLasF+ItOnT3d7A0ezef9s7KJl7Fq1O/58sfvuu2d8DNuJKt8KTLTedSnV5DjbQ9vzvJjd1cIwffp0V7fc2nHGGWfwwAMPpHU82+XIouCjjjrKJUWZBQsWcOihhwLZz4FJJN4ObumykQOrix495208z8vKbmGKiEVEREKU1xFxdBEOY3dg5q677mqTxVlYWNjukpZMsrGzxeapbe/STBQVFQEwevRoIFLT2FLq7bEws6Ztx6TnnnvO1Q23/UKtVGmqunXr5rLes5G1mI5Zs2bFjYizZfny5Xz00UdA5JwCfPfdd4G9Xzr++Mc/Av6e0MnWvS4tLXX/tjnvfKtN3NrixYuTep7N+VvRk1ReG5TLL7+cAw44APBLzdbW1rrrt6GhAcDV3F+8eDGHH354m+NYSVI7f5aJDf4qFMvqHzdunFsylStz5swB/J34Onfu7FYjJLNbWefOnd13lJVxjRcJW/9SW1vrrv1MFORy6UdBQUFKb3bxxRcD/uYMkNza0fY64jvvvDPtSkee53WYvZFqG22Y3PM8KioqgOSGccrKyjjvvPMA3PKX6OSggw8+GEh9vW4QbTSnn3662xTAtqq0Nic7rDxt2jQg0oHPnDkTSH05Sbba2KdPHx555BEgtmPJlHW2Q4YMSbtiVRDn0TbvsAScaHbN3nzzza6Skd0025aJAGeffTbgJ6s1NjZy2GGHAalvXhHktQr+8jD7jrRr19qQyDPPPAP47Z89ezbHHntszDGTlc022o3BhAkTADj11FPbVBlMlVXMmjBhgqsfnuq62iDO44oVKwDYZJNN3GN2M2QJV6eccgpbb711zOu6d+/ebkKW9StLliwB/CH6jnTURg1Ni4iIhCivI2LbwPrNN990i8STjYiXLVsG4Ib2bFPsb7/91kVjqQoyIt5tt91cVJhMveR+/fq5CMVYVFJfX8+FF14IkHJbg4wyevbs6YaMbEjMhs472jDcInyLgtesWcPAgQMB//8wWdlsY+/evQG/1nI6xS6soIxNU1ixl0yWKQVxHu1v75577olJlou2du1at4zMdsVqb7vIiy66iMmTJ6fyMZygI2KLbG0bREtyGjJkiBtGt7/BsrIyxo0bB/ijPAsXLgTg8MMPT7sGepBt3H///RkzZgzg7zRlEu1kZtekLfn85JNPAH9XvHQE0cbhw4cDkdHU6OHzTNn2l9afJEsRsYiISB7L64jYDBgwgKOPPhqAkSNHAh1HxBYR3n777em8ZVxB3Lkdc8wxQGQ/V1vukCz7P7D9RSdNmgT4u42kI+gowxImXn75ZQBXMm7q1Kltio707t3blce0iNmeX1NTEzeJIhlBtNGWy82cOTOl+eJ58+a5/VI7GhVIRZDncejQoS7ZLl1//vOfgch5TLdUZ9DXqs0fLlq0CPCvvcWLF/P+++8DkZEpiJ0HtyU0VVVVgF+jOh1BtzGRkpISfv3112wfNq4g2/jNN98kLBnb3NzsEl3Nl19+6dpt9bHt3L700kvMnTsX8K+JZHXUxnWiI45mmXw2NDBo0CDq6+sBP3OzoKDADVdms2pWkBfMNttsw3PPPQdEkoA6Mm3aNN59910gu1nDufrD79GjB+B/9oqKCr744ouYx/7yl7+0GX63TmvUqFEuwzNVQbaxtLTUDaNHb8YOkYxOSzYzDQ0NWSka31qQbSwsLHRD8qmu27SbLdtEPZMNO3J1rdoaYLt5+P/j2mdwj1llMJsyycaWnGF1xLkUdBsti9uSDK2iYnV1dZtkq6+//jrttdXt0dC0iIhIHlvnIuIw6e40IptttK31dt11V8aOHQv4yz5qamrc82bNmgX4y5wyWRet8xiRSRutcptVEdtll12AyNpRG6mx6QeLSMA/b9n43snVebRaw/379wfgySefZNNNNwX8EZo5c+ZQV1cHkJVKS0bXasT63kZFxCIiIiFSRJwC3blFqI35T22MUBvzn9qoiFhERCRU6ohFRERCpI5YREQkROqIRUREQpTTZC0RERGJpYhYREQkROqIRUREQqSOWEREJETqiEVEREKkjlhERCRE6ohFRERCpI5YREQkROqIRUREQqSOWEREJETqiEVEREKkjlhERCRE6ohFRERCpI5YREQkROqIRUREQqSOWEREJETqiEVEREKkjlhERCRE6ohFRERCpI5YREQkROqIRUREQqSOWEREJETqiEVEREKkjlhERCRE/wfG6idKQ3yvMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x72 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()  # Load the MNIST dataset.\n",
    "idx = np.where(y_test<1); x_test = x_test[idx]; y_test=y_test[idx]  # Get only digits 0 or 1.\n",
    "\n",
    "# Show some examples.\n",
    "_, ax = plt.subplots(1, 8, figsize=[8, 1])\n",
    "for i in range(8):\n",
    "    ax[i].imshow(x_test[i], cmap='gray')\n",
    "    ax[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: 98.96 percent!\n"
     ]
    }
   ],
   "source": [
    "# Predict 0 or 1 based on the center digit\n",
    "middle_pixel = 14  # the middle of a 28-by-28 array\n",
    "brightness_threshold = 128  # if array value is greater than this threshold, that means the pixel is filled in by the stroke\n",
    "y_pred = [1 if img[middle_pixel, middle_pixel] > brightness_threshold else 0 for img in x_test] \n",
    "\n",
    "# Evaluate accuracy, as a percent\n",
    "acc = 100*np.sum(y_pred == y_test)/len(y_test)\n",
    "print('The test accuracy is:', round(acc, 2), 'percent!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some sense, discriminative models map samples to classes, whereas with a generative models, we'd like to go into the reverse direction: given a class, we'd like to be able to generate new images. In fact, this definition is only a special case of a generative model, known as the *conditional generative model*. More generally, or perhaps as a first step, it would nice to have a model that can produce a sample that looks like it could have been found in our original dataset, *from any class*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/1_1_discriminative_generative.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the way we have described generative models above, you'll notice something interesting: a generative model must be able to generate *more than one output*! That's kind of strange -- a discriminative model only needs to output a class for a given image, but for generative models to be interesting, a single output probably won't be very useful: it needs to generate many different samples of the digit $7$ for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting, because that suggests we can't represent a generative model with a single deterministic function. If you think back to your middle-school algebra class, you'll remember a function can only have a single output. But we need some way to represent of our generative model to produce many different outputs. The standard way we do this -- though it is important to emphasize that this is a major modeling assumption -- is through *probabilistic models*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to define a random variable $X$ that takes values over the space of possible samples. We can say that this random variable has an associated random distribution $p_X(\\cdot)$, which is zero for unrealistic samples, and non-zero for realistic or possible samples. We have essentially represented the 'output' of the generative model as an *outcome* of the random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic models have several nice properties that make them great choices for generative models:\n",
    "\n",
    "* **Sampling:** as described already, probabilistic models have ability to generate multiple outputs, through sampling of the random variable $X$\n",
    "* **Learning:** classical statistics provides many methods (e.g. maximum likelihood estimation) to \"learn\" an optimal probability distribution from a family of probability distributions, given samples of the distribution. This methods can be leveraged to learn the appropriate generative model for a dataset.\n",
    "* **Conditioning:** it is well-understood how to adapt (\"condition\") probability distributions based on prior information. These methods can be used to adapt probability distribution when some properties are known about the sample that is intended to be generated (e.g. its label, or the top part of the image).\n",
    "* **Inference:** inference refers to estimating parts of the probability distribution, given other parts or given a particular samples. It allows us to ask questions of the probability distribution, which is often useful with generative models.\n",
    "* **Uncertainty Modeling:** Probabilistic models work nicely even when we have uncertainty in our samples, since uncertainty is usually modeled probabilistically anyways.\n",
    "\n",
    "Additionally, some generative models to provide a **likelihood** for a given sample. In other words, it is possible to evaluate $p_X(\\cdot)$ for a given sample. This can be useful (e.g. for anomaly detection) to determine whether a candidate sample could really have come from the training dataset or not. In other probabilistic methods, $p_X(\\cdot)$ is indirectly defined, so it is not possible to compute likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Kinds of Generative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't talked about how we go from a dataset of samples to a probability distribution $p_X()$ yet. That will the topic of many of the later notebooks, but here is a quick summary of some standard methods:\n",
    "\n",
    "* **Autoregressive Models** define an ordering over a the set of features in a high-dimensional class of objects (e.g. sequence of pixels in an image), and then learn how to generate each feature, conditioned on the values of the previous features. \n",
    "* **Variational Autoencoders**: define a latent representation for each high-dimensional object, parameterize a neural network to map from the latent representation to the high-dimensional object, and then jointly learn the parameters of the neural network and the latent representation.\n",
    "* **Flow Models**: also define a latent representation for each high-dimensional object, but use a very specific set of transformations to go from the latent representation to the high-dimensional objects, which allows calculations of exact likelihoods.\n",
    "* **Generative Adversarial Networks:** Use a discriminator neural network, rather than likelihood, to determine whether the quality of the  images produced by a generator is \"good enough\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-World Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"But how is this used in real life?\" If you're wondering that, here are just a few ways generative models are used by researchers and in industry. You'll see a lot more applications in the later notebooks!\n",
    "\n",
    "* **Speech synthesis**: synthesizing realistic speech from text is an important problem that is usually tackled through complex generative models.\n",
    "* **Anomaly detection**: a method that determines the likelihood of samples can be used to detect anomalies in all kinds of data\n",
    "* **Domain translation**: translating between different kinds of images (from photos to paintings, or MRIs to CT scans) has been achieved quite successfully with generative models.\n",
    "* **Interpolation**: being able to interpolate between images while producing a sequence of realistic images is useful for understanding/teaching the effect of changing underlying variations. This is a harder problem that it may look. We show in part(b) of this notebook how poorly direct linear interpolation of samples performs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
